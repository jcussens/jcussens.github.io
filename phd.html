<?xml version="1.0"  encoding="utf-8" ?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
"http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <title>PhD topics</title>
  </head>
  
  <body>

    <h3><a href="index.html">James Cussens</a>: Suggested PhD topics</h3>
    

      <hr />
      <h2>Learning causal models with latent variables</h2>

      <img src="verma_small.jpg" alt="Verma graph"/>

      
    <p>The graph above (which is known as the <em>Verma graph</em>)
      represents a causal model where X<sub>2</sub> is a cause of
      X<sub>4</sub> (via X<sub>3</sub>) but both X<sub>2</sub> and
      X<sub>4</sub> have a common cause L. If L is a <em>latent</em>
      variable, i.e. not one observed in data, then learning such a
      causal model is challenging, since we never get to see L! One
      approach to this difficult learning task is <em>constraint-based
      learning</em> where an algorithm infers the existence of latent
      variables from patterns of conditional independence between the
      observed variables.  </p>

    <p>An alternative approach is <em>score-based learning</em> where
    each candidate causal model (including those with latent
    variables) has a 'score' (typically penalised log-likelihood) and
    the task is to find a causal model with high (preferably maximal)
    score. There has been some recent progress in score-based learning
    for causal models with latent variables much of which has taken
    advantage of algebraic representations of causal models (see
      refs). In this existing work only quite small causal models have
    been learned. The goal of this project would be develop algorithms
    which can accurately learn larger causal models, even when there
    are latent variables. Developing algorithms which can incorporate
    prior knowledge would be advantageous. </p>

        <h4>References</h4>
    <ul>
      <li>
	Zhongyi Hu and Robin
	Evans. <a href="https://arxiv.org/abs/2208.10436">Towards
	standard imsets for maximal ancestral graphs</a>, March 2023.
      </li>
      <li>
	Bryan Andrews, Gregory F. Cooper, Thomas S. Richardson and
      Peter Spirtes. <a href="https://arxiv.org/abs/2207.08963">The
	  m-connecting imset and factorization for ADMG models</a>, July 2022.
      </li>
      <li>Rui Chen, Sanjeeb Dash and Tian
	Gao. <a href="http://proceedings.mlr.press/v139/chen21c/chen21c.pdf">Integer
	Programming for Causal Structure Learning in the Presence of
	Latent Variables</a> Proceedings of the 38th International
	Conference on Machine Learning (ICML21), PMLR 139:1550-1560,
	2021.
	</li>
    </ul>


    <hr />
    <h2>Learning directed acyclic graphs (DAGs)</h2>

    <p>When there are latent variables (see project above) learning
	causal models is particularly hard. If there are no latent
	variables the problem is easier and so there has been more
      progress. Nonetheless finding the 'best' directed acyclic graph
	(DAG) to explain some data is an NP-hard problem so it remains
      an active area of research.</p>

    <p>I have been using <em>integer programming</em> to attack this
      problem using a system
      called <a href="https://bitbucket.org/jamescussens/gobnilp/src/master/">GOBNILP</a>,
      which <a href="usinggobnilp.html">has been used by many other
      researchers</a>. GOBNILP is built on top of
      the <a href="https://scipopt.org">SCIP constraint integer
      programming
      system</a>.(A <a href="https://bitbucket.org/jamescussens/pygobnilp/src/master/">Python
      version</a> has also been developed.)  The basic idea of this
      approach is to encode the problem of learning a DAG as a
      constrained optimisation problem that a solver like SCIP
      (suitably extended) can solve. This has the advantage that
      constraints representing prior knowledge (e.g. variable A
      must/mustn't be a cause of variable B) can be incorporated
      reasonably easily.
    </p>

    <p>Since DAG learning is NP-hard there is always work to be done,
    e.g. increasing the size of learning problem where we can
    (reliably) find an optimal DAG in reasonable time, or handling
    user-supplied constraints more efficiently. The goal of this project is
    apply ideas from constraint and integer programming to DAG
    learning. This will suit someone who likes being able to apply
    theoretical ideas (from e.g. polyhedral geometry, see 2nd
    reference below) to design more
      efficient machine learning software.</p>
    
        <h4>References</h4>
    <ul>
     	<li>James
	Cussens. <a href="https://openreview.net/pdf?id=cZK80bf781S">
	Branch-Price-and-Cut for Causal Discovery</a> Proc. 2nd Conference on Causal Learning and Reasoning (CLeaR 2023), PMLR, 2023.
	</li>
	<li>
	  James Cussens, Matti Järvisalo, Janne H Korhonen and Mark
	  Bartlett. <a href="https://www.jair.org/index.php/jair/article/download/11041/26213/">Bayesian
	  network structure learning with integer programming:
	  Polytopes, facets and complexity</a> Journal of Artificial
	  Intelligence Research 58:185-229, 2017.
	</li>
	<li>
    James Cussens.
    <a href="https://arxiv.org/ftp/arxiv/papers/1202/1202.3713.pdf">Bayesian
	network learning with cutting planes</a>. Proc. 27th Conference on
	Uncertainty in Artificial Intelligence (UAI 2011), 2011.
  </li>
    </ul>


    


    <hr />
    <h2>Combinatorial optimisation in machine learning</h2>
    <p> There is a growing interest in methods which combine solving
combinatorial optimisation (CO) problems and machine learning. In some
cases machine learning is used to learn the best strategies for
to solving an NP-hard problem. In other cases, CO methods (e.g. integer
linear programming (ILP)) are used to solve hard machine learning
problems. In yet another combination CO solvers have been included as
a layer in a deep learning architecture. The goal of this project is
to explore this area further to allow further fruitful cross-fertilisation.
 </p>

    <h4>References</h4>
    <ul>
      <li>
	Quentin Cappart, Didier Chételat, Elias B. Khalil, Andrea
	Lodi, Christopher Morris and Petar
	Velickovic. <a href="https://www.jmlr.org/papers/v24/21-0449.html">Combinatorial
	Optimization and Reasoning with Graph Neural Networks</a>
	Journal of Machine Learning Research 24(130):1-61, 2023
      </li>
      <li><a href="https://sites.google.com/usc.edu/cpaior-2022/master_class">Bridging
	  the Gap between Machine Learning and Optimization</a>
      </li>
      <li>
	Dimitris Bertsimas and Jack
	Dunn. <a href="https://www.mit.edu/~dbertsim/papers/Machine%20Learning%20under%20a%20Modern%20Optimization%20Lens/Optimal_classification_trees_MachineLearning.pdf">Optimal
	  classification trees</a> Machine Learning 106:1039-1082, 2017 
      </li>
      <li>Claudio Gambella, Bissan Ghaddar and Joe
      Naoum-Sawaya. <a href="https://www.sciencedirect.com/science/article/abs/pii/S037722172030758X">Optimization
      problems for machine learning: A survey</a>, European Journal of
      Operational Research 290(3):807-828, 2021.
      </li>
    </ul>
    <hr />
    
  </body>
</html>
